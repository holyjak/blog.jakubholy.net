{:title
 "Notes On Automated Acceptance Testing (from the Continuous Delivery book)",
 :date "2015-01-08",
 :layout :post,
 :tags ["book" "DevOps"],
 :tags-orig ["book" "continuous_deployment"],
 :categories ["SW development" "Testing"],
 :extra-css #{}
}

++++
<em>(<a href="https://blog.iterate.no/2015/01/08/notes-on-automated-acceptance-testing-from-the-continuous-delivery-book/">Cross-posted from blog.iterate.no</a>)</em><br><br>These are my rather extensive notes from reading the chapter 8 on Automated Acceptance Testing in the <a title="Book at Amazon" href="https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912/">Continuous Delivery bible</a> by Humble and Farley. There is plenty of very good advice that I just had to take record of. Acceptance testing is an exciting and important subject. Why should you care about it? Because:<br><br><blockquote>We know from experience that without excellent automated acceptance test coverage, one of three things happens: Either a lot of time is spent trying to find and fix bugs at the end of the process when you thought you were done, or you spend a great deal of time and money on manual acceptance and regression testing, or you end up releasing poor-quality software.</blockquote><br><br><!--more--><br><br>(Ch8 focuses primarily on "functional" req., ch9 on "non-functional" or rather cross-functional requirements.)<br><br><ul>
    <li>acceptance tests are business-facing, story level test asserting it's complete and working run in a prod-like env; also serves as a regression test</li>
    <li>manual testing is expensive =&gt; done infrequently =&gt; defects discovered late when there is little time to fix them and we risk to introduce new refression defects</li>
    <li>Acc.T. put the app through a series of states =&gt; great for discovering threading problems, emergent behavior in event-driven apps, bugs due to architectural mistakes, env/config problems</li>
    <li>expensive if done poorly</li>
</ul><br><br><h3><a id="user-content-how-to-create-maintainable-acc-t-suites" class="anchor" href="#how-to-create-maintainable-acc-t-suites"></a>How to Create Maintainable Acc. T. Suites</h3><br><br><ol>
    <li>Good acceptance criteria ("<a href="https://en.wikipedia.org/wiki/INVEST_%28mnemonic%29">INVEST</a>" - especially valuable to users, testable)</li>
    <li>Layered implementation:
<ol>
    <li>Acceptance criteria (Given/When/Then) - as xUnit tests or with Concordion/FitNesse/...</li>
    <li>Test implementation - it's crucial that they use a (business) domain-specific language (DSL), no direct relation to UI/API, which would make it brittle</li>
    <li>Application driver layer - translates the DSL to interactions with the API/UI, extracts and returns results</li>
</ol>
</li>
    <li>Take care to keep test implementation efficient and well factored, especially wrt. managing state, handling timeouts, use of test doubles. Refactor.</li>
</ol><br><br><h4><a id="user-content-testing-against-gui" class="anchor" href="#testing-against-gui"></a>Testing against GUI</h4><br><br><ul>
    <li>it's most realistic but complex (to set up, ..), brittle, hard to extract results, impossible with GUI technologies that aren't testable</li>
    <li>if GUI is a thin layer of display-only code with no business/application logic, there is little risk in bypassing it and using the API it talks to directly - this is recommended whenever possible (plus, perhaps, few UI [smoke] tests)</li>
</ul><br><br><h3><a id="user-content-creating-acc-tests" class="anchor" href="#creating-acc-tests"></a>Creating Acc. Tests</h3><br><br><ul>
    <li>all (analyst, devs, testers) define acceptance criteria to ensure they all understand and testability</li>
</ul><br><br><h4><a id="user-content-acceptance-criteria-as-executable-specifications" class="anchor" href="#acceptance-criteria-as-executable-specifications"></a>Acceptance Criteria as Executable Specifications</h4><br><br>(See <a href="https://en.wikipedia.org/wiki/Behavior-driven_development">BDD</a>) - the plain text specifications are bound to actual tests so that they have to be kept up to date [JH: "Living Documentation"]<br><br><h3><a id="user-content-the-application-driver-layer" class="anchor" href="#the-application-driver-layer"></a>The Application Driver Layer</h3><br><br><ul>
    <li>provides business-level API and interacts with the application; f.ex. <code>admin_api.createUser("Dave")</code> or <code>app_api.placeOrder("Dave", {"product": "Chocolate", "quantity": "5kg"})</code> - that both translate into a complex sequence of interactions with the API/UI of the app</li>
    <li>tip: aliasing key values - createUser("Dave") actually creates a user with a random name but aliases it to "Dave" in the course of the test =&gt; readable test, unique data</li>
    <li>tip: defaults - test data are created with reasonable defaults so that a test only needs to set what it cares about - so <code>createUser</code> takes many optional parameters (tlf, email, balance, ...)</li>
    <li>a well done driver improves test reliability thanks to reuse - only 1/few places to fix on a change</li>
    <li>develop it iteratively, start with a few cases and simple tests, extend on-demand</li>
</ul><br><br><h4><a id="user-content-how-to-express-your-acceptance-criteria" class="anchor" href="#how-to-express-your-acceptance-criteria"></a>How to Express Your Acceptance Criteria</h4><br><br><ol>
    <li>Internal DSL, i.e. in your programming language (f.ex. JUnit tests using App. Driver) - simple(r), refactoring-frinedly, scary for business people</li>
    <li>External DSL - using FitNesse, Concordion etc. to record the acceptance criteria in plain text or HTML - easy to read and browse for non-tech people but more overhead to create, maintain, keep in synch with the tests [JH: This can be added on top of the internal DSL, pulling up test parameters]</li>
</ol><br><br>The Window Driver Pattern: Decoupling the Tests from the GUI<br><br><ul>
    <li>W.D. is the part of App.Driver responsible for interaction with the GUI</li>
    <li>may be split into multiple, for each distinct part of the application [standard coding best practice]</li>
    <li>write your tests so that if a new GUI is added, e.g. a gesture-based one, we only need to switch the driver without changing the test</li>
</ul><br><br><h3><a id="user-content-implementing-acceptance-tests" class="anchor" href="#implementing-acceptance-tests"></a>Implementing Acceptance Tests</h3><br><br><ul>
    <li>Topics: state, handling of asynchronicity and timeouts, data management, test doubles management etc.</li>
</ul><br><br><h4><a id="user-content-state-in-acceptance-testing" class="anchor" href="#state-in-acceptance-testing"></a>State in Acceptance Testing</h4><br><br><ul>
    <li>"[..] getting the application ready to exhibit the behavior under test is often the most difficult part of writing the test." p204</li>
    <li>we can't eliminate state; try to minimize dependency on complex state
<ul>
    <li>=&gt; resist the tendency to use prod DB dump; instead, maintain a controlled, minimal set of data; we want to focus on testing behavior, not data.</li>
    <li>this minimal coherent set of data should be represented as a collection of scripts; ideally they use the app's public API to put it into the correct state - less brittle than dumping data into the DB - see ch12</li>
</ul>
</li>
    <li>tests are ideally atomic, including independent =&gt; no hard-to-troubleshoot failures due to timing, possible to run in parallel
<ul>
    <li>an ideal test also creates all it needs and tidies up afterwards (this is admittedly difficult)</li>
    <li>tip: establish a transaction, roll back after the test - however this typically isn't possible if we treat acceptance testing as end-to-end testing as recommended [p205]</li>
</ul>
</li>
    <li>"The most effective approach to acceptance testing is to use the features of your application to isolate the scope of the tests." - f.ex. create a new user for every test, given independent user accounts</li>
    <li>if there is no way around tests sharing data, be very careful, they'll be very fragile; don't forget tear down</li>
    <li>worst possible case: unknown start state, impossible to clean up =&gt; make the tests <em>very</em> defensive (verify preconditions, ...)</li>
</ul><br><br><h4><a id="user-content-process-boundaries-encapsulation-and-testing" class="anchor" href="#process-boundaries-encapsulation-and-testing"></a>Process Boundaries, Encapsulation, and Testing</h4><br><br><ul>
    <li>preferably tests can act/verify without needing any priviledged access (back doors) to the app - don't succumb to the temptation to introduce such back doors, rather thing hard about design, improve modularity/encapsulation/verifiability [p206]</li>
    <li>if back doors the only option, 2 possibilities; both lead to brittle, high-maintenance code:
<ol>
    <li>Add test-specific API that enables you to modify the state/behavior of the app (e.g. switch WS for a stub for a particular call)</li>
    <li>React to "magic" data values (this is ugly, reserve it for your stubs)</li>
</ol>
</li>
</ul><br><br><h4><a id="user-content-managing-asynchrony-and-timeouts" class="anchor" href="#managing-asynchrony-and-timeouts"></a>Managing Asynchrony and Timeouts</h4><br><br><ul>
    <li>asynchrony arises f.ex. due to asynchronous communication, threads, transactions</li>
    <li>push asycnhronous behavior (wait for response, retries, ...) to the App Driver, expose synchronous API to the tests =&gt; easier to write tests, fewer places to tune; so in a test we will have f.ex. <code>sendAsyncMsg(m);verifyMsgProcessed();</code> and in the driver's sendAsyncMsg: <code>while(!timeout) if(pollResult) return; else sleep N; continue;</code></li>
    <li>tip: instead of waiting for MAX_TIMEOUT and then polling the result, retry polling it more frequently until response or timeout. If possible, replace polling with hooking into events generated by the system (i.e. register a listener) } both result in a faster response</li>
</ul><br><br><h4><a id="user-content-using-test-doubles" class="anchor" href="#using-test-doubles"></a>Using Test Doubles</h4><br><br><ul>
    <li>Automated acceptance tests are not the same as User Acceptance Tests, i.e. they shouldn't use (all) the real external systems, we need to ensure correct, known initial state and an external system we don't control prevents that [JH: unless it's stateless?]</li>
    <li>dilemma: integration is difficult to get it right and a common source of errors =&gt; test integration points carefully and effectively X external systems take out our control of the app's state and perhaps cannot handle the load generated by testing. One possible solution is to:
<ol>
    <li>Create and use test doubles for all ext. systems</li>
    <li>Create small test suites around every integration point using the real system</li>
</ol>
</li>
    <li>a benefit of test doubles is that they add points where we can control the behavior, simulate communication failures, simulate error responses or responses under load etc., that might be difficult to provoke in the real system</li>
    <li>minimize and contain the dependencies on ext. systems - preferably one gateway/adapter per system</li>
</ul><br><br><h4><a id="user-content-testing-external-integration-points" class="anchor" href="#testing-external-integration-points"></a>Testing External Integration Points</h4><br><br><ul>
    <li>these integration tests may need to run less frequently due to limitations of the target systems and might thus require a separate stage in the pipeline</li>
    <li>focus on likely problems; f.ex. in an evolving systems the schemas and contracts we rely upon will change and thus we want to test them</li>
    <li>"[..] there is usually a few obvious scenarios to simulate in most integrations" =&gt; do these, add more as defects are discovered. This approach isn't perfect but good wrt. cost/benefit.</li>
    <li>only test calls and data that you use and care about, not everything &lt;- cost/benefit</li>
</ul><br><br><h3><a id="user-content-the-acceptance-testing-stage" class="anchor" href="#the-acceptance-testing-stage"></a>The Acceptance Testing Stage</h3><br><br><ul>
    <li>fail the build if acceptance tests fail without a compromise; "stop the line"</li>
    <li>tip: record the interaction of the test and UI for troubleshooting, e.g. via <a href="https://www.unixuser.org/%7Eeuske/python/vnc2flv/index.html">Vnc2flv</a> [2/2010]</li>
    <li>"We know from experience that without excellent automated acceptance test coverage, one of three things happens: Either a lot of time is spent trying to find and fix bugs at the end of the process when you thought you were done, or you spend a great deal of time and money on manual acceptance and regression testing, or you end up releasing poor-quality software." p213</li>
</ul><br><br><h4><a id="user-content-keeping-acceptance-tests-green" class="anchor" href="#keeping-acceptance-tests-green"></a>Keeping Acceptance Tests Green</h4><br><br>Due to their slowness, devs don't wait for the result of acceptance tests and thus tend to ignore their failure =&gt; build-in discipline. If you let the tests rot, they will eventually die away or it will cost you more to fix them before the release (delayed feedback, lost context, ...).<br><br><h4><a id="user-content-deployment-tests" class="anchor" href="#deployment-tests"></a>Deployment Tests</h4><br><br>Ideal acceptance tests are atomic, set up and celan up their own data and thus have minimal dependency on existing state, and use public channels (API,..) instead of back doors. On the other hand, deployment tests are intended to verify, for the first time, that our deployment script works on a prod-like env. so they consist of a few smoke tests checking that the env. is configured as expected, communication links between components are up&amp;running. They run before functional acc. tests and fail the build immediately (instead for letting the acc. tests time out due to dead dependencies etc.). If we have other slow but important tests (f.ex. expelled from the commit stage), we can run them here as well.<br><br><h3><a id="user-content-acceptance-test-performance" class="anchor" href="#acceptance-test-performance"></a>Acceptance Test Performance</h3><br><br><ul>
    <li>being comprehensive and realistic (close to UI) is more important than speed; on large projects they often take few hours. Speedup tips below.</li>
</ul><br><br><h4><a id="user-content-refactor-common-tasks" class="anchor" href="#refactor-common-tasks"></a>Refactor Common Tasks</h4><br><br><ul>
    <li>factor out and reuse common tasks, especially in setup code, make the efficient</li>
    <li>setup via API is faster than via UI; sadly, sometimes it is unavoidable to preload test data to DB or use back door though we thus riks differences between these and what the UI would create</li>
</ul><br><br><h4><a id="user-content-share-expensive-resources" class="anchor" href="#share-expensive-resources"></a>Share Expensive Resources</h4><br><br>Ideally we share nothing but this is usually too slow; typically we share at least the instance of the app for all tests. On a project it was considered to share the instance of Selenium (=&gt; more complex code, risk of session leaks) but finally they rather parallelized the tests.<br><br><h4><a id="user-content-parallel-testing" class="anchor" href="#parallel-testing"></a>Parallel Testing</h4><br><br>Run multiple tests concurrently, perhaps against a single system instance - provided they're isolated.<br><br><h4><a id="user-content-using-compute-grids" class="anchor" href="#using-compute-grids"></a>Using Compute Grids</h4><br><br>Especially useful for single-user systems, very slow tests, or to simulate very many users. See f.ex. <a href="https://docs.seleniumhq.org/projects/grid/">Selenium Grid</a>.
++++
