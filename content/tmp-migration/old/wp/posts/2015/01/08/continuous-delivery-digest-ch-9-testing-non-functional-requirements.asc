{:title
 "Continuous Delivery Digest: Ch.9 Testing Non-Functional Requirements",
 :date "2015-01-08",
 :layout :post,
 :tags ["DevOps" "performance"],
 :tags-orig ["continuous_deployment" "performance"],
 :categories ["Testing"],
 :extra-css #{}
}

++++
<em>(<a href="https://blog.iterate.no/2015/01/08/notes-on-automated-acceptance-testing-from-the-continuous-delivery-book/">Cross-posted from blog.iterate.no</a>)</em><br><br>Digest of chapter 9 of the <a title="Book at Amazon" href="https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912/">Continuous Delivery bible</a> by Humble and Farley. See also the <a href="/2015/01/08/notes-on-automated-acceptance-testing-from-the-continuous-delivery-book/">digest of ch 8: Automated Acceptance Testing</a>.<br><br>("cross-functional" might be better as they too are crucial for functionality)<br><br><ul>
    <li>f.ex. security, usability, maintainability, auditability, configurability but especially capacity, throughput, performance</li>
    <li>performance = time to process 1 transaction (tx); throughput = #tx/a period; capacity = max throughput we can handle under a given load while maintaining acceptable response times.</li>
    <li>NFRs determine the architecture so we must define them early; all (ops, devs, testers, customer) should meet to estimate their impact (it costs to increase any of them and often they go contrary to each other, e.g. security and performance)</li>
    <li>das appropriate, you can either create specific stories for NFRs (e.g. capacity; =&gt; easier to prioritize explicitely) or/and add them as requirements to feature stories</li>
    <li>if poorly analyzed then they constrain thinking, lead to overdesign and inappropriate optimization</li>
    <li>only ever optimize based on facts, i.e. realistic measurements (if you don't know it: developers are really terrible at guessing the source of performance problems)</li>
</ul><br><br>A strategy to address capacity problems:<br><br><!--more--><br><br><ol>
    <li>Decide upon an architecture; beware process/network boundaries and I/O in general</li>
    <li>Apply stability/capacity patterns and avoid antipatterns - see <a href="https://pragprog.com/book/mnee/release-it">Release It!</a></li>
    <li>Other than that, avoid premature optimization; prefer clear, simple code; never optimize without a proof it is necessary</li>
    <li>Make sure your algorithms and data structures are suitable for your app (O(n) etc.)</li>
    <li>Be extremely careful about threading (-&gt; "blocked threads anti-pattern")</li>
    <li>Create automated tests asserting the desired capacity; they also will guide you when fixing failures</li>
    <li>Only profile to fix issues identified by tests</li>
    <li>Use real-world capacity measures whenever possible - measure in your prod system (# users, patterns of behavior, data volumes, ...)</li>
</ol><br><br><h3><a id="user-content-measuring-capacity" class="anchor" href="#measuring-capacity"></a>Measuring Capacity</h3><br><br>There are different possible tests, f.ex.:<br><br><ul>
    <li>Scalability testing - how does the response time of an individual request and # concurrent users changes as we add more servers, services, or threads?</li>
    <li>Longevity t. - see performance changes when running for a long time - detect memory leaks, stability problems</li>
    <li>Throughput t. - #tx/messages/page hits per second</li>
    <li>Load t. - capacity as functional of load to and beyond the prod-like volumes; this is the most common</li>
    <li>it's vital to use realistic scenarios; on the contrary, technical benchmark-style measurements (# reads/s from DB,..) can be sometimes useful to guard against specific problems, to optimize specific areas, or to choose a technology</li>
    <li>systems do many things so it's important to run different capacity tests in parallel; it's impossible to replicate prod traffic =&gt; use traffic analysis, experience, intuition to achieve as close a simulation as possible</li>
</ul><br><br><h4><a id="user-content-how-to-define-success-or-failure" class="anchor" href="#how-to-define-success-or-failure"></a>How to Define Success or Failure</h4><br><br><ul>
    <li>tip: collect measurements (absolute values, trends) during the testing and present them in a graphical form to gain insight into what happened</li>
    <li>too strict limits will lead to intermittent failures (f.ex. when network overloaded by another operation) X too relaxed limits =&gt; won't discover a partial drop in capacity =&gt;
<ol>
    <li>Aim for stable, reproducible results - isolate the test env as much as possible</li>
    <li>Tune the pass threshold up once it passes at a minimum acceptable level; back down if it starts failing after a commit due to well-understood and acceptable reason</li>
</ol>
</li>
</ul><br><br><h4><a id="user-content-capacity-testing-environment" class="anchor" href="#capacity-testing-environment"></a>Capacity-Testing Environment</h4><br><br><ul>
    <li>replicates Prod as much as possible; extrapolation from a different environment is highly speculative, unless based on good measurements. "Configuration changes tend to have nonlinear effect on capacity characteristics." p234</li>
    <li>an exact replica of Prod sometimes impossible or not sensible (small project, capacity little important, or when prod has 100s of servers) =&gt; capacity testing can be done on a subset of prod servers as a part of Canary Releasing, see p263</li>
    <li>scaling is rarely linear, even if the app is designed for it; if test env is a scaled-down prod, do few scalings runs to measure the size effect</li>
    <li>saving money on a downscaled test env is a false economy if capacity is critical; no matter what it won't be able to find all issues and it will be expensive to fix them later - see the storu on p236</li>
</ul><br><br><h4><a id="user-content-automating-capacity-testing" class="anchor" href="#automating-capacity-testing"></a>Automating Capacity Testing</h4><br><br><ul>
    <li>it's expensive but if important, it must be a part of the deployment pipeline</li>
    <li>these tests are complex, fragile, easily broken with minor changes</li>
    <li>Ideal tests: use real-world scenarios; predefine success threshold; relatively short duration to finish in a reasonable time; robust wrt. change to improve maintainability; composable into larger-scale scenarios so that we can simulate real-world patterns of use; repeatable and runnable sequentially or in parallel =&gt; suitable both for load and longevity testing</li>
    <li>start with some existing (robust and realistic) acceptance tests, adapt them for capacity testing - add success threshold and auditability to scale up</li>
</ul><br><br>Goals:<br><br><ol>
    <li>Creat realistic, prod-like load (in form and volume)</li>
    <li>Test realistic but pathological real-life loading scenarios, i.e. not just the happy path; tip: identify the most expensive transactions and double/triple their ratio</li>
</ol><br><br>To scale up, you can record the communication generated by acceptance tests, postprocess it to scale up (multiply, insert unique data where necessary), reply at high volume<br><br><ul>
    <li>Question: Where to record and play back:
<ol>
    <li>UI - realistic but impractical for 10,000s users (and expensive)</li>
    <li>Service/public API (e.g. HTTP req.)</li>
    <li>Lower-level API (such as a direct call to the service layer or DB)</li>
</ol>
</li>
</ul><br><br><h4><a id="user-content-testing-via-ui" class="anchor" href="#testing-via-ui"></a>Testing via UI</h4><br><br><ul>
    <li>Not suitable for high-volume systems, when too many clients are necessary to generate a high load (partially due to UI client [browser] overhead); also expensive to run many machines</li>
    <li>UI condenses a number of actions (clicks, selections) into few interactions with back-end (e.g. 1 form submission) that has a more stable API. To answer: are we interested in performance of the clients or of the back-end.</li>
    <li>"[..] we generally prefer to avoid capacity testing through the UI." - unless the UI itself or the client-server interaction are of a concern</li>
</ul><br><br><h4><a id="user-content-recording-interactions-against-a-service-or-public-api" class="anchor" href="#recording-interactions-against-a-service-or-public-api"></a>Recording Interactions against a Service or Public API</h4><br><br><ul>
    <li>run acceptance tests, record in/outputs (e.g. SOAP XML, HTTP), replace what must vary with placeholders (e.g. ${ORDER_ID}), create test data, merge the two</li>
    <li>Recommended compromise: Aim to change as little as possible between instances of a test - less coupling between the test and test data, more flexible, less fragile. Ex.: unique orderId, customerId but same product, quantity.</li>
</ul><br><br><h4><a id="user-content-using-capacity-test-stubs-to-develop-tests" class="anchor" href="#using-capacity-test-stubs-to-develop-tests"></a>Using Capacity Test Stubs To Develop Tests</h4><br><br>In high-performance systems testing may fail because the tests themselves do not run fast enough. To discover this case, run them originally against a no-op stub of the application.<br><br><h3><a id="user-content-adding-capacity-tests-to-the-deployment-pipeline" class="anchor" href="#adding-capacity-tests-to-the-deployment-pipeline"></a>Adding Capacity Tests to the Deployment Pipeline</h3><br><br><ul>
    <li>beware that warm-up time may be necessary (JIT, ...)</li>
    <li>for known hot spots, you can simple "guard tests" already to the commit stage</li>
    <li>typically we run them separately from acceptance tests - they've different environment needs, perhaps are long-running, we want to avoid undesirable interactions between acceptance and capacity tests; acceptance test stage may include a few performance smoke tests</li>
</ul><br><br><h3><a id="user-content-other-benefits-of-capacity-tests" class="anchor" href="#other-benefits-of-capacity-tests"></a>Other Benefits of Capacity Tests</h3><br><br>Composable, scenario-based tests enable us to simulate complex interactions, together with prod-like env we can<br><br><ul>
    <li>reproduce complex prod defects</li>
    <li>detect/debug memory leaks</li>
    <li>evaluate impact of garbage collection (GC); tune GC</li>
    <li>tune app config and 3rd party app (OS, AS, DB, ...) config</li>
    <li>simulate worst-day scenarios</li>
    <li>evaluate different solutions to a complex problem</li>
    <li>simulate integration failures</li>
    <li>measure scalability with different hardware configs</li>
    <li>load-test communication with external systems even though the tests were originally designed for stubbed interfaces</li>
    <li>rehears rollback</li>
    <li>and many more ...</li>
</ul>
++++
