{:title "Most interesting links of Mars '12",
 :date "2012-03-31",
 :layout :post,
 :tags ["testing" "design" "methodology" "DevOps"],
 :tags-orig ["agile" "cloud" "design" "management" "Testing" "trends"],
 :categories ["General" "Testing" "Top links of month"],
 :extra-css #{}
}

++++
<h2>Recommended Readings</h2>
<ul>
	<li><a href="https://thoughtworks.fileburst.com/assets/thoughtworks-tech-radar-march-2012-us-color.pdf">ThoughtWorks Technology Radar 3/2012</a> - including apps with embedded servlet containers (assess), health check pages for webapp monitoring, <a href="/2012/01/18/how-to-create-maintainable-acceptance-tests/">testing at the appropriate level</a> (adopt), JavaScript micro-framewors (trial, see <a href="https://Microjs.com/">Microjs.com</a>), Gradle over Maven (e.g. thanks to flexibility), <a href="https://opensocial.org/">OpenSocial</a> for data &amp; content sharing between (enterprise) apps (assess), Clojure (before in asses) and CoffeeScript on trial (Scala very close to adopt), JavaScript as a 1st class language (adopt), single-threaded servers with aync I/O (Node.js, <a href="https://github.com/webbit/webbit">Webbit</a> for Java [http/websocket], ...; assess).</li>
	<li>Jez Humble: <a href="https://www.informit.com/articles/article.aspx?p=1833567">Four Principles of Low-Risk Software Releases</a> - how to make your releases safer by making them incremental (versioned artifacts instead of overwritting, expand &amp; contract DB scripts, versioned APIs, releasing to a subset of customers first), separating software deployment from releasing it so that end-users can use it (=&gt; you can do smoke tests, canary releasing, dark launching [feature in place but not visible to users, already doing something]; includes feature toggles [toggle on only for somebody, switch off new buggy feature, ...]), delivering features in smaller batches (=&gt; more frequently, smaller risk of any individual release thanks to less stuff and easier roll-back/forward), and optimizing for resiliance (=&gt; ability to provision a running production system to a known good state in predictable time - crucial when stuff fails).</li>
	<li><a href="https://blog.incubaid.com/2012/03/28/the-game-of-distributed-systems-programming-which-level-are-you/">The Game of Distributed Systems Programming. Which Level Are You?</a> (via Kent Beck) - we start with a naive approach to distributed systems, treating them as just a little different local systems, then (painfully) come to understand the <a href="https://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing">fallacies of distributed programming</a> and start to program explicitely for the distributed environment leveraging asynchronous messaging and (often functional) languages with good support for concurrency and distribution. We suffer by random, subtle, non-deterministic defects and try to separate and restrict non-determinism by becoming purely functional ... . Much recommended to anybody dealing with distributed systems (i.e. everybody, nowadays). The discussion is worth reading as well.</li>
	<li><a href="https://rgordon.co.uk/blog/2012/03/05/shapes-dont-draw/#.T2oidI2SwnE.dzone">Shapes Don’t Draw</a> - thought-provoking criticism of inappropriate use of OOP, which leads to bad and inflexible code. Simplification is OK as long as the domain is equally simple - but in the real world shapes do not draw themselves. (And Trades don't decide their price and certainly shouldn't reference services and a database.)</li>
	<li><a href="https://www.grisha.ru/cmm/cimm.htm">Capability Im-Maturity Model</a> (via Markus Krüger) - everybody knows CMMI, but it’s useful to know also the negative directions an organization can develop in. Defined by Capt. Tom Schorsch in 1996, building on Anthony Finkelstein's paper <a href="https://www.cs.ucl.ac.uk/staff/A.Finkelstein/papers/immaturity.pdf">A Software Process Immaturity Model</a>.</li>
	<li>Cynefin: <a href="https://hbr.org/2007/11/a-leaders-framework-for-decision-making/ar/1">A Leader’s Framework for Decision Making</a> - an introduction into the <a href="https://en.wikipedia.org/wiki/Cynefin">Cynefin cognitive framework</a> - the key point is that we encounter 5 types of contexts differing by the predictability of effects and each of them requires a different management style, using the wrong one is a recipe for a disaster. Quote:
<blockquote>The framework sorts the issues facing leaders into five contexts defined by the nature of the relationship between cause and effect. Four of these—simple, complicated, complex, and chaotic—require leaders to diagnose situations and to act in contextually appropriate ways. The fifth—disorder—applies when it is unclear which of the other four contexts is predominant.</blockquote>
</li>
	<li><a href="https://scrummaster.no/?p=571">Et spørsmål om kompleksitet</a> (Norwegian). Key ideas mixed with my own: Command &amp; control management in the traditional Ford way works very well - but only in stable domains with clear cause-and-effect relationships (i.e. the Simple context of Cynefin). But many tasks today have lot of uncertanity and complexity and deal with creating new, never before seen things. We try to lead projects as if they were automobile factories while often they are more like research - and researchers cannot plan when they will make a breakthrough. Most of the new development of IT systems falls into the Complex context of Cynefin - there is lot of uncertanity, no clear answers, we cannot forsee problems, and have to base our progress on empirical experience and leverage <a href="https://en.wikipedia.org/wiki/Emergence">emergence</a> (emergent design, ..).</li>
	<li><a href="https://blogs.captechconsulting.com/blog/chris-wash/the-economics-developer-testing">The Economics of Developer Testing</a> - a very interesting reflection on the cost and value of testing and what is enough tests. Tests cost to develop and maintain (and different tests cost differently, the more complex the more expensive). Not having tests costs too - usually quite a lot. To find the right ballance between tests and code and different types of tests we must be aware of their cost and benefits, both short &amp; long term. Worth reading, good links. (Note: We often tend to underestimate the cost of not having good tests. Much more then you might think.)</li>
</ul>
<h2>Links to Keep</h2>
<ul>
	<li><a href="https://www.lukew.com/ff/entry.asp?1514">Proved Patterns for Creating Responsive Web UIs</a> (adapting to the user device size - PC, mobile, ..): Mostly Fluid (fluid grids, down-scaling images,decreasing margins and eventually stacking columns below each other on smaller screens), Column Drop, Layout Shifter (diff. layout for large/medium/small screens), ... .</li>
	<li><a href="https://h30565.www3.hp.com/t5/Feature-Articles/16-Linux-Server-Monitoring-Commands-You-Really-Need-To-Know/ba-p/1936">16 Linux Server Monitoring Commands You Really Need To Know</a> - some elementary such as top, iostat, and netstat and some more dashboard-like such as <a href="https://nmon.sourceforge.net/pmwiki.php" rel="nofollow" target="_blank">nmon</a> (Nigel's Monitor) and the stats collecting <a href="https://www.thegeekstuff.com/2011/03/sar-exampl" rel="nofollow" target="_blank">sar</a>.</li>
</ul>
<h2>Quotes</h2>
Kent Beck answering a <a href="https://stackoverflow.com/a/153565">question about how much testing to do</a> (highlighted by me):
<blockquote><strong>I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence</strong> (I suspect <strong>this level of confidence is high compared to industry standards</strong>, but that could just be hubris). If I don't typically make a kind of mistake (like setting the wrong variables in a constructor), I don't test for it. I do tend to make sense of test errors, so I'm extra careful when I have logic with complicated conditionals. When coding on a team, I modify my strategy to carefully test code that we, collectively, tend to get wrong.<br><br>Different people will have different testing strategies based on this philosophy, but that seems reasonable to me given the immature state of understanding of how tests can best fit into the inner loop of coding. Ten or twenty years from now we'll likely have a more universal theory of which tests to write, which tests not to write, and how to tell the difference. In the meantime, experimentation seems in order.</blockquote>

++++
